###############################
logstash-syslog:
  enabled: true
  antiAffinity: soft

  resources:
    limits:
      cpu: "4500m"
      memory: "1536Mi"

  fullnameOverride: logstash-syslog #From Syslog to Kafka
  replicas: 13
  image: gcr.io/its-artifact-commons/logstash-loki
  imageTag: develop-3a0839b

  service:
    type: NodePort
    ports:
    - port: 5140
      #protocol: UDP
      targetPort: 5140
      nodePort: 30140

  extraEnvs:
    - name: LS_JAVA_OPTS
      value: "-Xmx256m -Xms256m"

    - name: DUMMY
      value: "072"

    - name: CTI_RATE_LIMIT # Request/Sec
      value: "5" # Request/sec
    - name: CTI_ERROR_RATE_LIMIT # Request/5 minutes
      value: "10"
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name

  logstashConfig:
    logstash.yml: |
      http.host: "0.0.0.0"
      pipeline.ecs_compatibility: "disabled"
      pipeline.workers: 4

  extraVolumeMounts:
    - name: scripts
      mountPath: /scripts/
      readOnly: true
    - name: configs
      mountPath: /configs/
      readOnly: true
  extraVolumes:
    - name: scripts
      configMap:
        name: ruby-filter-cm
    - name: configs
      configMap:
        name: ruby-config-cm

  logstashPipeline:
    logstash.conf: |      
      input {
        syslog {
          port => 5140
        }
      }

      filter {
        ruby {
          code => "event.set('destination', 'kafka')"
        }

        ruby {
          path => "/scripts/etl.rb"
          script_params => {}
        }
      }

      output {
        #stdout { codec => rubydebug }
        kafka {
          bootstrap_servers => "kafka-soc-kafka-bootstrap.kafka-soc.svc.cluster.local:9092"
          codec => json
          topic_id => "kafka-soc-topic"
        }
      }

###############################
logstash-loki:
  enabled: true
  antiAffinity: soft

  fullnameOverride: logstash-loki #From kafka to Loki
  replicas: 17
  image: gcr.io/its-artifact-commons/logstash-loki
  imageTag: develop-3a0839b

  extraEnvs:
    - name: ES_USER
      valueFrom:
        secretKeyRef:
          name: es-soc-basic-authen
          key: username
    - name: ES_PASSWORD
      valueFrom:
        secretKeyRef:
          name: es-soc-basic-authen
          key: password
    - name: DUMMY
      value: "012"
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name

  logstashConfig:
    logstash.yml: |
      http.host: "0.0.0.0"
      pipeline.ecs_compatibility: "disabled"
      #log.level: "debug"
      # Dummy value - [{{ .Files.Get "scripts/etl.rb" | sha256sum }}] for pod restart
      # Dummy value - [{{ .Files.Get "configs/fields-map.cfg" | sha256sum }}] for pod restart

  extraVolumeMounts:
    - name: scripts
      mountPath: /scripts/
      readOnly: true
    - name: configs
      mountPath: /configs/
      readOnly: true

  extraVolumes:
    - name: scripts
      configMap:
        name: ruby-filter-cm
    - name: configs
      configMap:
        name: ruby-config-cm

  logstashPipeline:
    logstash.conf: |
      input {
        kafka {
          bootstrap_servers => "kafka-soc-kafka-bootstrap.kafka-soc.svc.cluster.local:9092"
          group_id => "for-loki"
          consumer_threads => 1
          topics => ["kafka-soc-topic", "kafka-soc-topic-beat"]
          codec => "json"
          type => "log"
          auto_offset_reset => "earliest"
        }
      }
      
      filter {
        ruby {
          code => "event.set('destination', 'loki')"
        }

        ruby {
          path => "/scripts/etl.rb"
          script_params => {}
        }

        clone {
          # Create field name 'type' = es
          clones => ["es"]
        }

        if [type] == "es" {
          prune {
            interpolate => true
            whitelist_names => [ "type", "mt", "@timestamp", "message" ]
          }
        }
      }

      output {
        #stdout { codec => rubydebug }        

        # To use ES change type back to "es"
        if [type] == "es" {
          elasticsearch {
            hosts => [ "https://es-soc-es-http.es-soc.svc.cluster.local:9200" ]
            ssl_certificate_verification => false
            user => "${ES_USER}"
            password => "${ES_PASSWORD}"
            action => "create"

            ecs_compatibility => "v8"
            http_compression => "true"

            index => "rtarf-events-soc-%{[mt][ads_ts_yyyy]}%{[mt][ads_ts_mm]}%{[mt][ads_ts_dd]}-%{[mt][pod_name_loki]}"
            template_name => "rtarf-events-soc"
            template => "/configs/es-template.json"
            template_overwrite => "true"
          }
        }

        if [type] == "log" {
          loki {
            message_field => "message"
            include_fields => [
              "type",
              "is_delayed_log",
              "destination",
              "ads_category",
              "ads_alert_by_dstip",
              "ads_alert_by_srcip",
              "ads_alert_by_domain"
            ]
            url => "http://loki-soc-gateway.loki-soc.svc.cluster.local/loki/api/v1/push"
          }
        }
      }

###############################
logstash-geoip:
  enabled: true
  antiAffinity: soft

  fullnameOverride: logstash-geoip #From kafka to GeoIP map
  replicas: 3
  image: gcr.io/its-artifact-commons/logstash-loki
  imageTag: develop-3a0839b

  extraEnvs:
    - name: DUMMY
      value: "005"

  logstashConfig:
    logstash.yml: |
      http.host: "0.0.0.0"
      pipeline.ecs_compatibility: "disabled"

  extraVolumeMounts:
    - name: scripts
      mountPath: /scripts/
      readOnly: true

  extraVolumes:
    - name: scripts
      configMap:
        name: geoip-filter-cm

  logstashPipeline:
    logstash.conf: |
      input {
        kafka {
          bootstrap_servers => "kafka-soc-kafka-bootstrap.kafka-soc.svc.cluster.local:9092"
          group_id => "for-geoip-map"
          consumer_threads => 1
          topics => ["kafka-soc-topic"]
          codec => "json"
          type => "log"
          auto_offset_reset => "earliest"
        }
      }

      filter {
        ruby {
          path => "/scripts/geoip.rb"
          script_params => {}
        }
      }

      output {
        #stdout { codec => rubydebug }

        syslog {
          id => "geoip_attack_map"
          host => "geoip-map-development-attack-map-feeder.geoip-map.svc.cluster.local"
          port => 6514
          protocol => "tcp"
        }
      }

###############################
logstash-beat:
  enabled: true
  antiAffinity: soft

  fullnameOverride: logstash-beat
  replicas: 1
  image: gcr.io/its-artifact-commons/logstash-loki
  imageTag: develop-3a0839b

  service:
    type: NodePort
    ports:
    - port: 5044
      #protocol: UDP
      targetPort: 5044
      nodePort: 30144

  extraEnvs:
    - name: LS_JAVA_OPTS
      value: "-Xmx256m -Xms256m"

    - name: DUMMY
      value: "016"

  logstashConfig:
    logstash.yml: |
      http.host: "0.0.0.0"
      pipeline.ecs_compatibility: "disabled"

  extraVolumeMounts:
    - name: scripts
      mountPath: /scripts/
      readOnly: true

  extraVolumes:
    - name: scripts
      configMap:
        name: beat-filter-cm

  logstashPipeline:
    logstash.conf: |
      input {
        beats {
          port => 5044
        }
      }

      filter {
        ruby {
          path => "/scripts/beat.rb"
          script_params => {}
        }
      }

      output {
        #stdout { codec => json_lines }

        kafka {
          bootstrap_servers => "kafka-soc-kafka-bootstrap.kafka-soc.svc.cluster.local:9092"
          codec => json
          topic_id => "kafka-soc-topic-beat"
        }
      }
